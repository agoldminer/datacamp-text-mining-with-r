---
title: "chapter-3-visualizing-sentiment"
author: "Xiaotian Sun"
date: "16/02/2020"
output: html_document
---

## 3. Visualizing Sentiment

##### 3.1 Parlor trick or worthwhile? (video)

- Interesting visuals

  - simple to interpret
  
  - confirm or elucidata data
  
  - context for the audience
  
  - appropriate type e.g. line charts for time, bars for amounts
  
  - avoid word clouds (they are cliche).
  
- Tracking sentiment over time

**Sentiment timeline** - a way of displaying sentiment values in chronological order. It is typically a graphic design showing time periods, such as months, as the X axis and the sentiment values as Y axis values either as a line or series of bars.

- Simple frequency analysis

It is suitable to make frequency analysis part of **EDA** (exploratory data analysis), when doing a text mining project.

##### 3.2 Real insight? 

##### 3.3 Unhappy ending? Chronological polarity 

Sometimes you want to track sentiment over time. For example, during an ad campaign you could track brand sentiment to see the campaign's effect. You saw a few examples of this at the end of the last chapter.

In this exercise you'll recap the workflow for exploring sentiment over time using the novel Moby Dick. One should expect that happy moments in the book would have more positive words than negative. Conversely dark moments and sad endings should use more negative language. You'll also see some tricks to make your sentiment time series more visually appealing.

Recall that the workflow is:

- Inner join the text to the lexicon by word.

- Count the sentiments by line.

- Reshape the data so each sentiment has its own column.

- (Depending upon the lexicon) Calculate the polarity as positive score minus negative score.

- Draw the polarity time series.

This exercise should look familiar: it extends Bing tidy polarity: Call me Ishmael (with ggplot2)!.

***Instruction 1:***

- `inner_join()` the pre-loaded tidy version of Moby Dick, `moby`, to the `bing` lexicon.

  - Join by the `"term"` column in the text and the `"word"` column in the lexicon.

- Count by `sentiment` and `index`.

- Reshape so that each sentiment has its own column using `spread()` with the column `sentiment` and the counts column called `n`.

  - Also specify `fill = 0` to fill out missing values with a zero.

- Using `mutate()` add two columns: `polarity` and `line_number`.

  - Set `polarity` equal to the positive score minus the negative score.
  
  - Set `line_number` equal to the row number using the `row_number()` function.

```{r eval=FALSE}
moby_polarity <- moby %>%
  # Inner join to the lexicon
  inner_join(bing, by = c("term" = "word")) %>%
  # Count by sentiment, index
  count(sentiment, index) %>%
  # Spread sentiments
  spread(sentiment, n, fill = 0) %>%
  mutate(
    # Add polarity field
    polarity = positive - negative,
    # Add line number field
    line_number = row_number()
  )
```

***Instruction 2:***

Create a sentiment time series with `ggplot()`.

- Pass in `moby_polarity` to the `data` argument.

- Call `aes()` and pass in `line_number` and `polarity` without quotes.

- Add a smoothed curve with `geom_smooth()`.

- Add a red horizontal line at zero by calling `geom_hline()`, with parameters `0` and `"red"`.

- Add a title with `ggtitle()` set to `"Moby Dick Chronological Polarity"`.

```{r eval=FALSE}
# Plot polarity vs. line_number
ggplot(moby_polarity, aes(x = line_number, y = polarity)) + 
  # Add a smooth trend curve
  geom_smooth() +
  # Add a horizontal line at y = 0
  geom_hline(yintercept = 0, color = "red") +
  # Add a plot title
  ggtitle("Moby Dick Chronological Polarity") +
  theme_gdocs()
```

##### 3.4 Word impact, frequency analysis 

One of the easiest ways to explore data is with a frequency analysis. Although not difficult, in sentiment analysis this simple method can be surprisingly illuminating. Specifically, you will build a barplot. In this exercise you are once again working with `moby` and `bing` to construct your visual.

To get the bars ordered from lowest to highest, you will use a trick with factors. `reorder()` lets you change the order of factor levels based upon another scoring variable. In this case, you will reorder the factor variable `term` by the scoring variable `polarity`.

***Instruction 1:***

```{r eval=FALSE}
moby_tidy_sentiment <- moby %>% 
  # Inner join to bing lexicon by term = word
  inner_join(bing, by = c("term" = "word")) %>% 
  # Count by term and sentiment, weighted by count
  count(term, sentiment, wt = count) %>%
  # Spread sentiment, using n as values
  spread(sentiment, n, fill = 0) %>%
  # Mutate to add a polarity column
  mutate(polarity = positive - negative)

# Review
moby_tidy_sentiment
```

***Instruction 2:***

- Use `filter()` on ·moby_tidy_sentiment· to keep rows where the absolute `polarity` is greater than or equal to 50. `abs()` gives you absolute values.

- `mutate()` a new vector pos_or_neg with an `ifelse()` function checking if `polarity > 0` then declare the document `"positive"` else declare it `"negative"`.

```{r eval=FALSE}
moby_tidy_pol <- moby_tidy_sentiment %>% 
  # Filter for absolute polarity at least 50 
  filter(abs(polarity) >= 50) %>% 
  # Add positive/negative status
  mutate(
    pos_or_neg = ifelse(polarity > 0, "positive", "negative")
  )
```

***Instruction 3:***

- Using `moby_tidy_pol`, plot `polarity` vs. `term`, reordered by `polarity` (`reorder(term, polarity))`, filled by `pos_or_neg`.

- Inside `element_text()`, rotate the x-axis text `90` degrees by setting `angle = 90` and shifting the vertical justification with `vjust = -0.1`.

```{r eval=FALSE}
# Plot polarity vs. (term reordered by polarity), filled by pos_or_neg
ggplot(moby_tidy_pol, aes(reorder(term, polarity), polarity,  fill = pos_or_neg)) +
  geom_col() + 
  ggtitle("Moby Dick: Sentiment Word Frequency") + 
  theme_gdocs() +
  # Rotate text and vertically justify
  theme(axis.text.x = element_text(angle = 90, vjust = -0.1))
```

##### 3.5 Introspection using sentiment analysis (video)

- qdap's polarity for subsetting corpora

- grep(), grepl()

- or is represented as | in the grep() function

##### 3.6 Divide & conquer: Using polarity for a comparison cloud 

Now that you have seen how polarity can be used to divide a corpus, let's do it! This code will walk you through dividing a corpus based on sentiment so you can peer into the information in subsets instead of holistically.

Your R session has `oz_pol` which was created by applying `polarity()` to "The Wonderful Wizard of Oz."

For simplicity's sake, we created a simple custom function called `pol_subsections()` which will divide the corpus by polarity score. First, the function accepts a data frame with each row being a sentence or document of the corpus. The data frame is subset anywhere the polarity values are greater than or less than 0. Finally, the positive and negative sentences, non-zero polarities, are pasted with parameter `collapse` so that the terms are grouped into a single corpus. Lastly, the two documents are concatenated into a single vector of two distinct documents.

> pol_subsections <- function(df) {
  x.pos <- subset(df$text, df$polarity > 0)
  x.neg <- subset(df$text, df$polarity < 0)
  x.pos <- paste(x.pos, collapse = " ")
  x.neg <- paste(x.neg, collapse = " ")
  all.terms <- c(x.pos, x.neg)
  return(all.terms)
}

At this point you have omitted the neutral sentences and want to focus on organizing the remaining text. In this exercise we use the `%>%` operator again to forward objects to functions. After some simple cleaning use `comparison.cloud()` to make the visual.

***Instruction 1:***

- Extract the bits you need from oz_pol.
 
  - Call `select()`, declaring the first column `text` as `text.var` which is the raw text. The second column `polarity` should refer to the polarity scores `polarity.`

- Now apply `pol_subsections()` to `oz_df`. Call the new object `all_terms`.

- To create `all_corpus` apply `VectorSource()` to `all_terms` and then `%>%` to `VCorpus()`.

```{r eval=FALSE}
oz_df <- oz_pol$all %>%
  # Select text.var as text and polarity
  select(text = text.var, polarity = polarity)

# Apply custom function pol_subsections()
all_terms <- pol_subsections(oz_df)

all_corpus <- all_terms %>%
  # Source from a vector
  VectorSource() %>% 
  # Make a volatile corpus 
  VCorpus()
```

***Instruction 2:***

- Create a `term-document` matrix, `all_tdm`, using `TermDocumentMatrix()` on `all_corpus`.

  - Add in the parameters 
  `control = list(removePunctuation = TRUE, stopwords = stopwords(kind = "en")))`.

  - Then` %>% `to `as.matrix()` and `%>%` again to `set_colnames(c("positive", "negative"))`.

```{r eval=FALSE}
all_tdm <- TermDocumentMatrix(
  # Create TDM from corpus
  all_corpus,
  control = list(
    # Yes, remove the punctuation
    removePunctuation = TRUE,
    # Use English stopwords
    stopwords = stopwords(kind = "en")
  )
) %>%
  # Convert to matrix
  as.matrix() %>%
  # Set column names
  set_colnames(c("positive", "negative"))
```

***Instruction 3:***

- Apply `comparison.cloud()` to `all_tdm` with parameters `max.words = 50`, and `colors = c("darkgreen","darkred")`.

```{r eval=FALSE}
comparison.cloud(
  # Create plot from the all_tdm matrix
  all_tdm,
  # Limit to 50 words
  max.words = 50,
  # Use darkgreen and darkred colors
  colors = c("darkgreen","darkred")
)
```

##### 3.7 Emotional introspection 

In this exercise you go beyond subsetting on positive and negative language. Instead you will subset text by each of the 8 emotions in Plutchik's emotional wheel to construct a visual. With this approach you will get more clarity in word usage by mapping to a specific emotion instead of just positive or negative.

Using the `tidytext` subjectivity lexicon, "nrc", you perform an `inner_join()` with your text. The "nrc" lexicon has the 8 emotions plus positive and negative term classes. So you will have to drop positive and negative words after performing your `inner_join()`. One way to do so is with the negation, `!`, and `grepl()`.

The "Global Regular Expression Print Logical" function, `grepl()`, will return a True or False if a string pattern is identified in each row. In this exercise you will search for positive OR negative using the `|` operator, representing "or" as shown below. Often this straight line is above the enter key on a keyboard. Since the `!` negation precedes `grepl()`, the T or F is switched so the `"positive|negative"` is dropped instead of kept.

>Object <- tibble %>%
  filter(!grepl("positive|negative", column_name))

Next you apply `count()` on the identified words along with `spread()` to get the data frame organized.

`comparison.cloud()` requires its input to have row names, so you'll have to convert it to a base-R `data.frame`, calling `data.frame()` with the `row.names` argument.

***Instruction 1:***

- `inner_join()` `moby` to `nrc`.

- Using `filter()` with a negation (`!`) and `grepl()` search for `"positive|negative"`. The column to search is called sentiment.

- Use `count()` to count by sentiment and term.

- Reshape the data frame with `spread()`, passing in `sentiment`, `n`, and `fill = 0`.

- Convert to plain data frame with `data.frame()`, making the `term` column into rownames.

- Examine `moby_tidy` using `head()`.

```{r eval=FALSE}
moby_tidy <- moby %>%
  # Inner join to nrc lexicon
  inner_join(nrc, by = c("term" = "word")) %>% 
  # Drop positive or negative
  filter(!grepl("positive|negative", sentiment)) %>% 
  # Count by sentiment and term
  count(sentiment, term) %>% 
  # Spread sentiment, using n for values
  spread(sentiment, n, fill = 0)  %>% 
  # Convert to data.frame, making term the row names
  data.frame(row.names = "term")

# Examine
head(moby_tidy)
```

***Instruction 2:***

- Using `moby_tidy`, draw a `comparison.cloud()`.
  - Limit to `50` words.
  - Increase the title size to `1.5`.

```{r eval=FALSE}
# From previous step
moby_tidy <- moby %>%
  inner_join(nrc, by = c("term" = "word")) %>% 
  filter(!grepl("positive|negative", sentiment)) %>% 
  count(sentiment, term) %>% 
  spread(sentiment, n, fill = 0) %>% 
  data.frame(row.names = "term")
  
# Plot comparison cloud
comparison.cloud(moby_tidy, max.words = 50, title.size = 1.5)
```

##### 3.8 Compare & contrast stacked bar chart 

Another way to slice your text is to understand how much of the document(s) are made of positive or negative words. For example a restaurant review may have some positive aspects such as "the food was good" but then continue to add "the restaurant was dirty, the staff was rude and parking was awful." As a result, you may want to understand how much of a document is dedicated to positive vs negative language. In this example it would have a higher negative percentage compared to positive.

One method for doing so is to `count()` the positive and negative words then divide by the number of subjectivity words identified. In the restaurant review example, "good" would count as 1 positive and "dirty," "rude," and "awful" count as 3 negative terms. A simple calculation would lead you to believe the restaurant review is 25% positive and 75% negative since there were 4 subjectivity terms.

Start by performing the `inner_join()` on a unified tidy data frame containing 4 books, Agamemnon, Oz, Huck Finn, and Moby Dick. Just like the previous exercise you will use `filter(`) and `grepl()`.

To perform the `count()` you have to group the data by book and then sentiment. For example all the positive words for Agamemnon have to be grouped then tallied so that positive words from all books are not mixed. Luckily, you can pass multiple variables into `count()` directly.

***Instruction 1:***

- Inner join `all_books` to the lexicon, `nrc`.

- Filter to keep rows where `sentiment` contains `"positive"` or `"negative"`. That is, use `grepl()` on the `sentiment` column, checking without the negation so that `"positive|negative"` are kept.

- Count by book and sentiment.

```{r echo=FALSE, include=FALSE}
library(tidytext)
library(tidyverse)
library(qdap)
all_books <- read_rds("all_books.rds") #%>% filter(book %in% c("agamemnon", "oz", "huck_finn", "moby_dick"))
nrc <- get_sentiments("nrc")
```

```{r }
# Review tail of all_books
tail(all_books)

# Count by book & sentiment
books_sent_count <- all_books %>%
  # Inner join to nrc lexicon
  inner_join(nrc, by = c("term" = "word")) %>% 
  # Keep only positive or negative
  filter(grepl("positive|negative", sentiment)) %>% 
  # Count by book and by sentiment
  count(book, sentiment)
  
# Review entire object
books_sent_count
```

***Instruction 2:***

- Group `books_sent_count` by line.

- Mutate to add a column named `percent_positive`. This should e calculated as `100` times `n` divided by the sum of `n`.

```{r}
book_pos <- books_sent_count %>%
  # Group by book
  group_by(book) %>% 
  # Mutate to add % positive column 
  mutate(percent_positive = 100 * n / sum(n) )
```

***Instruction 3:***

- Using `book_pos`, plot `percent_positive` vs. `book`, using `sentiment` as the fill color.

- Add a column layer with `geom_col()`.

```{r}
# Plot percent_positive vs. book, filled by sentiment
ggplot(book_pos, aes(y = percent_positive, x = book, fill = sentiment)) +  
  # Add a col layer
  geom_col()
```

##### 3.9 Interpreting visualizations (video)

- kernel density plot

- Box plot

- Radar chart

- Treemap

##### 3.10 Kernel density plot 

Now that you learned about a kernel density plot you can create one! Remember it's like a smoothed histogram but isn't affected by binwidth. This exercise will help you construct a kernel density plot from sentiment values.

In this exercise you will plot 2 kernel densities. One for Agamemnon and another for The Wizard of Oz. For both you will perform an `inner_join()` with the "afinn" lexicon. Recall the "afinn" lexicon has terms scored from -5 to 5. Once in a tidy format, both books will retain words and corresponding scores for the lexicon.

After that, you need to row bind the results into a larger data frame using `bind_rows()` and create a plot with `ggplot2`.

From the visual you will be able to understand which book uses more positive versus negative language. There is clearly overlap as negative things happen to Dorothy but you could infer the kernel density is demonstrating a greater probability of positive language in the Wizard of Oz compared to Agamemnon.

We've loaded `ag` and `oz` as tidy versions of Agamemnon and The Wizard of Oz respectively, and created `afinn` as a subset of the `tidytext` `"afinn"` lexicon.

***Instruction 1:***

- Inner join `ag` to the lexicon, `afinn`, assigning to `ag_afinn`.

- Do the same for The Wizard of Oz. This is the same code, but starting with the `oz` dataset and assigning to `oz_afinn`.

- Use `bind_rows()` to combine `ag_afinn` to `oz_afinn`. Set the `.id` argument to `"book"` to create a new column with the name of each book.

```{r eval=FALSE}
ag_afinn <- ag %>% 
  # Inner join to afinn lexicon
  inner_join(afinn, by = c("term" = "word"))

oz_afinn <- oz %>% 
  # Inner join to afinn lexicon
  inner_join(afinn, by = c("term" = "word"))
 
# Combine
all_df <- bind_rows(agamemnon = ag_afinn, oz = oz_afinn, .id = "book")
```

***Instruction 2:***

- Using `all_df`, plot `value`, using `book` as the `fill` color.

- Set the `alpha` transparency to `0.3`.

```{r eval=FALSE}
# Plot value, filled by book
ggplot(all_df, aes(x = value, fill = book)) + 
  # Set transparency to 0.3
  geom_density(alpha = 0.3) + 
  theme_gdocs() +
  ggtitle("AFINN Score Densities")
```

##### 3.11 Box plot 

An easy way to compare multiple distributions is with a box plot. This code will help you construct multiple box plots to make a compact visual.

In this exercise the `all_book_polarity` object is already loaded. The data frame contains two columns, `book` and `polarity`.`= It comprises all books with qdap's `polarity()` function applied. Here are the first 3 rows of the large object.

|book |	polarity|
|---|---|
|14	| huck	| 0.2773501
|22	| huck	| 0.2581989
|26	| huck	| -0.5773503

This exercise introduces `tapply()` which allows you to apply functions over a ragged array. You input a vector of values and then a vector of factors. For each factor, value combination the third parameter, a function like `min()`, is applied. For example here's some code with `tapply()` used on two vectors.

>f1 <- as.factor(c("Group1", "Group2", "Group1", "Group2"))
stat1 <- c(1, 2, 1, 2)
tapply(stat1, f1, sum)

The result is an array where `Group1` has a value of 2 (1+1) and `Group2` has a value of 4 (2+2).

***Instruction:***

- Since it's already loaded, examine the all_book_polarity with `str()`.

- Using `tapply()`, pass in `all_book_polarity$polarity`, `all_book_polarity$book` and the `summary()` function. This will print the summary statistics for the 4 books in terms of their `polarity()` scores. You would expect to see Oz and Huck Finn to have higher averages than Agamemnon or Moby Dick. Pay close attention to the median.

- Create a box plot with `ggplot()` by passing in `all_book_polarity`.

  - Aesthetics should be `aes(x = book, y = polarity)`.
  
  - Using a `+` add the `geom_boxplot()` with `col = "darkred"`. Pay close attention to the dark line in each box representing median.
 
  - Next add another layer called `geom_jitter()` to add points for each of the words.

```{r echo=FALSE}
all_book_polarity <- readRDS("all_book_polarity.rds")
library(ggplot2)
```

```{r}
# Examine
str(all_book_polarity)

# Summary by document
tapply(all_book_polarity$polarity,all_book_polarity$book, summary)

# Box plot
ggplot(all_book_polarity, aes(x = book, y = polarity)) +
  geom_boxplot(fill = c("#bada55", "#F00B42", "#F001ED", "#BA6E15"), col = "darkred") +
  geom_jitter(position = position_jitter(width = 0.1, height = 0), alpha = 0.02) +
  ggtitle("Book Polarity")
```

##### 3.12 Radar chart 

Remember Plutchik's wheel of emotion? The NRC lexicon has the 8 emotions corresponding to the first ring of the wheel. Previously you created a `comparison.cloud()` according to the 8 primary emotions. Now you will create a radar chart similar to the wheel in this exercise.

A `radarchart` is a two-dimensional representation of multidimensional data (at least 3). In this case the tally of the different emotions for a book are represented in the chart. Using a radar chart, you can review all 8 emotions simultaneously.

As before we've loaded the "nrc" lexicon as `nrc` and moby_huck which is a combined tidy version of both Moby Dick and Huck Finn.

In this exercise you once again use a negated `grepl()` to remove `"positive|negative"` emotional classes from the chart. As a refresher here is an example:

>object <- tibble %>%
  filter(!grepl("positive|negative", column_name))

This exercise reintroduces `spread()` which rearranges the tallied emotional words. As a refresher consider this raw data called `datacamp`.

|people	|food	|like|
|---|---|---|
|Nicole	|bread	|78|
|Nicole	|salad	|66|
|Ted	|bread	|99|
|Ted	|salad |21|

If you applied spread() as in spread(datacamp, people, like) the data looks like this.

|food	|Nicole	|Ted|
|---|---|---|
|bread	|78	|99|
|salad	|66	|21|

***Instruction 1:***
- Review moby_huck with `tail(`).

- `inner_join()` `moby_huck` and `nrc`.

- Next, `filter()` negating `"positive|negative"` in the sentiment column. Assign the result to `books_pos_neg`.

- After `books_pos_neg` is forwarded to `group_by()` with `book` and `sentiment`. Then `tally()` the object with an empty function.

- Then `spread(`) the `books_tally` by the `book` and `n` column.

- Review the `scores` data.

```{r echo=FALSE}
moby_huck <- all_books %>% filter(book %in% c("moby_dick", "huck_finn"))
```

```{r}
# Review tail of moby_huck
tail(moby_huck)

scores <- moby_huck %>% 
  # Inner join to lexicon
  inner_join(nrc, by = c("term" = "word")) %>% 
  # Drop positive or negative
  filter(!grepl("positive|negative", sentiment)) %>% 
  # Count by book and sentiment
  count(book, sentiment) %>% 
  # Spread book, using n as values
  spread(book, n)

# Review scores
scores
```

***Instruction 2:***

- Call `chartJSRadar()` on `scores` which is an `htmlwidget` from the `radarchart` package.

```{r echo=FALSE}
library(radarchart)
```

```{r}
# From previous step
scores <- moby_huck %>% 
  inner_join(nrc, by = c("term" = "word")) %>% 
  filter(!grepl("positive|negative", sentiment)) %>% 
  count(book, sentiment) %>%
  spread(book, n)
  
# JavaScript radar chart
chartJSRadar(scores)
```

##### 3.13 Treemaps for groups of documents

Often you will find yourself working with documents in groups, such as author, product or by company. This exercise lets you learn about the text while retaining the groups in a compact visual. For example, with customer reviews grouped by product you may want to explore multiple dimensions of the customer reviews at the same time. First you could calculate the `polarity()` of the reviews. Another dimension may be length. Document length can demonstrate the emotional intensity. If a customer leaves a short "great shoes!" one could infer they are actually less enthusiastic compared to a lengthier positive review. You may also want to group reviews by product type such as women's, men's and children's shoes. A treemap lets you examine all of these dimensions.

For text analysis, within a treemap each individual box represents a document such as a tweet. Documents are grouped in some manner such as author. The size of each box is determined by a numeric value such as number of words or letters. The individual colors are determined by a sentiment score.

After you organize the tibble, you use the `treemap` library containing the function `treemap()` to make the visual. The code example below declares the data, grouping variables, size, color and other aesthetics.

>treemap(
  data_frame,
  index = c("group", "individual_document"),
  vSize = "doc_length",
  vColor = "avg_score",
  type = "value",
  title = "Sentiment Scores by Doc",
  palette = c("red", "white", "green")
)

The pre-loaded `all_books` object contains a combined tidy format corpus with 4 Shakespeare, 3 Melville and 4 Twain books. Based on the treemap you should be able to tell who writes longer books, and the polarity of the author as a whole and for individual books.

***Instruction 1:***

Calculate each book's length in a new object called `book_length` using `count()` with the `book` column.

```{r}
book_length <- all_books %>%
  # Count number of words per book
  count(book)
  
# Examine the results
book_length
```

***Instruction 2:***

- Inner join `all_books` to the lexicon, `afinn`.

- Group by `author` and `book`.

- Use `summarize()` to calculate the `mean_value` as the `mean()` of value.

- Inner join again, this time to `book_length`. Join by the `book` column.

```{r echo=FALSE}
afinn <- get_sentiments("afinn")
```

```{r}
book_tree <- all_books %>% 
  # Inner join to afinn lexicon
  inner_join(afinn, by = c("term" = "word")) %>% 
  # Group by author, book
  group_by(author, book) %>%
  # Calculate mean book value
  summarize(mean_value = mean(value)) %>% 
  # Inner join by book
  inner_join(book_length, by = "book")

# Examine the results
book_tree
```

***Instruction 3:***

- Draw a treemap, setting the following arguments.

- Use the `book_tree` from the previous step.

- Specify the aggregation `index` columns as `"author"` and `"book"`.

- Specify the vertex size column, `vSize`, as `"n"`.

- Specify the vertex color column, `vColor`, as `"mean_value"`.

- Specify a direct mapping from `vColor` to the palette by setting `type = "value"`.

```{r echo=FALSE}
library("treemap")
```

```{r}
treemap(
  # Use the book tree
  book_tree,
  # Index by author and book
  index = c("author", "book"),
  # Use n as vertex size
  vSize = "n",
  # Color vertices by mean_value
  vColor = "mean_value",
  # Draw a value type
  type = "value",
  title = "Book Sentiment Scores",
  palette = c("red", "white", "green")
)
```