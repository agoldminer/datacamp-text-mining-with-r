---
title: "chapter-2-sentiment-analysis-the-tidytext-way"
author: "Xiaotian Sun"
date: "16/02/2020"
output: html_document
---

## 2. Sentiment Analysis the Tidytext Way

##### 2.1 Plutchik's wheel of emotion, polarity vs. sentiment (video)

##### 2.2 One theory of emotion 

##### 2.3 DTM vs. tidytext matrix 

The `tidyverse` is a collection of R packages that share common philosophies and are designed to work together. This chapter covers some tidy functions to manipulate data. In this exercise you will compare a DTM to a tidy text data frame called a tibble.

Within the tidyverse, each observation is a single row in a data frame. That makes working in different packages much easier since the fundamental data structure is the same. Parts of this course borrow heavily from the tidytext package which uses this data organization.

For example, you may already be familiar with the `%>%` operator from the `magrittr` package. This forwards an object on its left-hand side as the first argument of the function on its right-hand side.

In the example below, you are forwarding the `data` object to `function1()`. Notice how the parentheses are empty. This in turn is forwarded to `function2()`. In the last function you don't have to add the data object because it was forwarded from the output of `function1()`. However, you do add a fictitious parameter, `some_parameter` as `TRUE`. These pipe forwards ultimately create the `object`.

>object <- data %>% 
           function1() %>%
           function2(some_parameter = TRUE)
           
To use the `%>%` operator, you don't necessarily need to load the `magrittr` package, since it is also available in the `dplyr` package. `dplyr` also contains the functions `inner_join()` (which you'll learn more about later) and `count()` for tallying data. The last function you'll need is `mutate()` to create new variables or modify existing ones.

>object <- data %>%
  mutate(new_Var_name = Var1 - Var2)

or to modify a variable

>object <- data %>%
  mutate(Var1 = as.factor(Var1))
  
You will also use tidyr's `spread()` function to organize the data with each row being a line from the book and the positive and negative values as columns.

|index |	negative |	positive |
|  ----  | ----  |  ----  | 
| 42 | 	2	 | 0 | 
| 43 | 	0	 | 1 | 
| 44 | 	1	 | 0 | 

To change a DTM to a tidy format use `tidy()` from the `broom` package.

> tidy_format <- tidy(Document_Term_Matrix)

This exercise uses text from the Greek tragedy, Agamemnon. Agamemnon is a story about marital infidelity and murder. You can download a copy here.

***Instruction:***

We've already created a clean `DTM` called ag_dtm for this exercise.

- Create `ag_dtm_m` by applying `as.matrix()` to ag_dtm.

- Using brackets, `[` and `]`, index `ag_dtm_m` to row `2206`.

- Apply `tidy()` to ag_dtm. Call the new object `ag_tidy`.

- Examine `ag_tidy` at rows `[831:835, ]` to compare the tidy format. You will see a common word from the examined part of ag_dtm_m in step 2.

```{r eval=FALSE}
# As matrix
ag_dtm_m <- as.matrix(ag_dtm)

# Examine line 2206 and columns 245:250
ag_dtm_m[2206, 245:250]

# Tidy up the DTM
ag_tidy <- tidy(ag_dtm)

# Examine tidy with a word you saw
ag_tidy[831:835, ]
```

##### 2.4 Getting Sentiment Lexicons

So far you have used a single lexicon. Now we will transition to using three, each measuring sentiment in different ways.

The `tidytext` package contains a function called `get_sentiments` which along with the [`textdata`] package allows you to download & interact well researched lexicons. Here is a small section of the loughran lexicon.

|Word	|Sentiment|
|  ----  | ----  |
|abandoned	| negative |
|abandoning	| negative | 
|abandonment	| negative |
|abandonments	| negative |
|abandons	| negative |

This lexicon contains 4150 terms with corresponding information. We will be exploring other lexicons but the structure & method to get them is similar.

Let's use `tidytext` with `textdata` to explore other lexicons' word labels!

***Instruction 1:***

- Use `get_sentiments()` to obtain the `"afinn"` lexicon, assigning to `afinn_lex`.

- Review the overall `count()` of value in `afinn_lex`.

```{r echo=FALSE}
library(tidytext)
```

```{r eval=FALSE}
# Subset to AFINN
afinn_lex <- get_sentiments("afinn")

# Count AFINN scores
afinn_lex %>% 
  count(value)
```

***Instruction 2:***

Do the same again, this time with the `"nrc"` lexicon. That is,

- get the sentiments, assigning to `nrc_lex`, then

- count the `sentiment` column, assigning to `nrc_counts`.

```{r eval=FALSE}
# Subset to nrc
nrc_lex <- get_sentiments("nrc")

# Make the nrc counts object
nrc_counts <- nrc_lex%>%
count(sentiment)
```

***Instruction 3:***

- Create a ggplot labeling the y-axis as `n` vs. x-axis of `sentiment`.

- Add a `col` layer using `geom_col()`. (This is like `geom_bar()`, but used when you've already summarized with `count()`.)

```{r eval=FALSE}
# From previous step
nrc_counts <- get_sentiments("nrc") %>% 
  count(sentiment)
  
# Plot n vs. sentiment
ggplot(nrc_counts, aes(x = sentiment, y = n)) +
  # Add a col layer
  geom_col() #+theme_gdocs()
```

##### 2.5 Bing lexicon with an inner join explanation (video)

- inner_join()

- anti_join()

##### 2.6 Bing tidy polarity: Simple example 

Now that you understand the basics of an inner join, let's apply this to the "Bing" lexicon. Keep in mind the `inner_join()` function comes from `dplyr` and the lexicon object is obtained using `tidytext`'s `get_sentiments()` function'.

The Bing lexicon labels words as positive or negative. The next three exercises let you interact with this specific lexicon. To use `get_sentiments()` pass in a string such as "afinn", "bing", "nrc", or "loughran" to download the specific lexicon.

The inner join workflow:

- Obtain the correct lexicon using get_sentiments().

- Pass the lexicon and the tidy text data to inner_join().

- In order for `inner_join()` to work there must be a shared column name. If there are no shared column names, declare them with an additional parameter, `by` equal to `c` with column names like below.

>object <- x %>% 
    inner_join(y, by = c("column_from_x" = "column_from_y"))
    
- Perform some aggregation and analysis on the table intersection.

***Instruction:***

We've loaded `ag_txt` containing the first 100 lines from Agamemnon and `ag_tidy` which is the tidy version.

- For comparison, use `polarity()` on `ag_txt`.

- Get the "bing" lexicon by passing that string to get_sentiments().

- Perform an `inner_join()` with `ag_tidy` and `bing`.

  - The word columns are called `"term"` in `ag_tidy` & `"word"` in the lexicon, so declare the `by` argument.
  
  - Call the new object `ag_bing_words`.

- Print `ag_bing_words`, and look at some of the words that are in the result.

- Pass `ag_bing_words` to `count()` of `sentiment` using the pipe operator, %>%. Compare the `polarity()` score to sentiment count ratio.

```{r eval=FALSE}
# Qdap polarity
polarity(ag_txt)

# Get Bing lexicon
bing <- get_sentiments("bing")

# Join text to lexicon
ag_bing_words <- inner_join(ag_tidy, bing, by = c("term" = "word"))

# Examine
ag_bing_words

# Get counts by sentiment
ag_bing_words %>%
  count(sentiment)
```

##### 2.7 Bing tidy polarity: Count & spread the white whale 

In this exercise you will apply another `inner_join()` using the `"bing"` lexicon.

Then you will manipulate the results with both `count()` from `dplyr` and `spread()` from `tidyr` to learn about the text.

The `spread()` function spreads a key-value pair across multiple columns. In this case the key is the sentiment & the values are the frequency of positive or negative terms for each line. Using `spread()` changes the data so that each row now has positive and negative values, even if it is 0.

***Instruction:***

In this exercise, your R session has `m_dick_tidy` which contains the book Moby Dick and `bing`, containing the lexicon similar to the previous exercise.

- Perform an `inner_join()` on `m_dick_tidy` and `bing`.

  - As before, join the `"term"` column in `m_dick_tidy` to the `"word"` column in the lexicon.
  - Call the new object `moby_lex_words`.

- Create a column `index`, equal to `as.numeric()` applied to `document`. This occurs within `mutate()` in the tidyverse.

- Create `moby_count` by forwarding `moby_lex_words` to `count()`, passing in `sentiment, index`.

- Generate `moby_spread` by piping `moby_count` to `spread()` which contains `sentiment`, `n`, and `fill = 0`.

```{r eval=FALSE}
# Inner join
moby_lex_words <- inner_join(m_dick_tidy, bing, by = c("term" = "word"))

moby_lex_words <- moby_lex_words %>%
  # Set index to numeric document
  mutate(index = as.numeric(document))

moby_count <- moby_lex_words %>%
  # Count by sentiment, index
  count(sentiment, index)

# Examine the counts
moby_count

moby_spread <- moby_count %>%
  # Spread sentiments
  spread(sentiment, n, fill = 0)

# Review the spread data
moby_spread
```

##### 2.8 Bing tidy polarity: Call me Ishmael (with ggplot2)! 

The last Bing lexicon exercise! In this exercise you will use the pipe operator (`%>%`) to create a timeline of the sentiment in Moby Dick. In the end you will also create a simple visual following the code structure below. The next chapter goes into more depth for visuals.

> ggplot(spread_data, aes(index_column, polarity_column)) +
  geom_smooth()

***Instruction 1:***

- Inner join moby to the bing lexicon.

  - Call `inner_join()` to join the tibbles.
  - Join by the term column in the text and the word column in the lexicon.

- Count by `sentiment` and `index`.

- Reshape so that each sentiment has its own column.

  - Call `spread()`.

  - The key column (to split into multiple columns) is `sentiment`.

  - The value column (containing the counts) is `n`.
  
   - Also specify `fill = 0` to fill out missing values with a zero.

- Use `mutate()` to add the `polarity` column. Define it as the difference between the `positive` and `negative` columns.

```{r eval=FALSE}
moby_polarity <- moby %>%
  # Inner join to lexicon
  inner_join(bing, by = c("term" = "word")) %>%
  # Count the sentiment scores
  count(sentiment, index) %>% 
  # Spread the sentiment into positive and negative columns
  spread(sentiment, n, fill = 0) %>%
  # Add polarity column
  mutate(polarity = positive - negative)
```

***Instruction 2:***

- Using moby_polarity, plot polarity vs. index.

- Add a smooth trend layer by calling geom_smooth() with no arguments.

```{r eval=FALSE}
# Plot polarity vs. index
ggplot(moby_polarity, aes(y = polarity, x = index)) + 
  # Add a smooth trend curve
  geom_smooth()  
```

##### 2.9 AFINN & NRC methodologies in more detail (video)

##### 2.10 AFINN: I'm your Huckleberry 

Now we transition to the AFINN lexicon. The AFINN lexicon has numeric values from 5 to -5, not just positive or negative. Unlike the Bing lexicon's `sentiment`, the AFINN lexicon's sentiment score column is called `value`.

As before, you apply `inner_join()` then `count()`. Next, to sum the scores of each line, we use dplyr's `group_by()` and `summarize()` functions. The `group_by()` function takes an existing data frame and converts it into a grouped data frame where operations are performed "by group". Then, the `summarize()` function lets you calculate a value for each group in your data frame using a function that aggregates data, like `sum()` or `mean()`. So, in our case we can do something like

> 29data_frame %>% 
    group_by(book_line) %>% 
    summarize(total_score = sum(book_line))
    
In the tidy version of Huckleberry Finn, line 9703 contains words "best", "ever", "fun", "life" and "spirit". "best" and "fun" have AFINN scores of 3 and 4 respectively. After aggregating, line 9703 will have a total score of 7.

In the tidyverse, `filter()` is preferred to `subset()` because it combines the functionality of `subset()` with simpler syntax. Here is an example that `filter()`s data_frame where some value in `column1` is equal to `24`. Notice the column name is not in quotes.

> filter(data_frame, column1 == 24)

The `afinn` object contains the AFINN lexicon. The `huck` object is a tidy version of Mark Twain's Adventures of Huckleberry Finn for analysis.

Line 5400 is All the loafers looked glad; I reckoned they was used to having fun out of Boggs. Stopwords and punctuation have already been removed in the dataset.

***Instruction 1:***

- Run the code to look at line 5400, and see the sentiment scores of some words.

- `inner_join()` `huck` to the `afinn` lexicon.

  - Remember `huck` is already piped into the function so just add the lexicon.

  - Join by the `term` column in the text and the `word` column in the lexicon.

- Use `count()` with `value` and `line` to tally/count observations by group.

  - Assign the result to `huck_afinn`.

```{r eval=FALSE}
# See abbreviated line 5400
huck %>% filter(line == 5400)

# What are the scores of the sentiment words?
afinn %>% filter(word %in% c("fun", "glad"))

huck_afinn <- huck %>% 
  # Inner Join to AFINN lexicon
  inner_join(afinn, by = c("term" = "word")) %>%
  # Count by value and line
  count(value, line)
```

***Instruction 2:***

- Get the total sentiment score by line forwarding `huck_afinn` to `group_by()` and passing `line` without quotes.

- Create `huck_afinn_agg` using `summarize()`, setting `total_value` equal to the `sum()` of `value * n`.

- Use `filter()` on `huck_afinn_agg` and `line == 5400` to review a single line.

```{r eval=FALSE}
huck_afinn_agg <- huck_afinn %>% 
  # Group by line
  group_by(line) %>%
  # Sum values times n (by line)
  summarize(total_value = sum(value * n))
  
huck_afinn_agg %>% 
  # Filter for line 5400
  filter(line == 5400)
```

***Instruction 3:***

- Create a sentiment timeline. Pass `huck_afinn_agg` to the data argument of `ggplot()`.

  - Then specify the `x` and `y` within `aes()` as `line` and `total_value` without quotes.
  
  - Add a layer with `geom_smooth()`.

```{r eval=FALSE}
# Plot total_value vs. line
ggplot(huck_afinn_agg, aes(x = line, y = total_value)) + 
  # Add a smooth trend curve
  geom_smooth() 
```

##### 2.11 The wonderful wizard of NRC 

Last but not least, you get to work with the NRC lexicon which labels words across multiple emotional states. Remember Plutchik's wheel of emotion? The NRC lexicon tags words according to Plutchik's 8 emotions plus positive/negative.

In this exercise there is a new operator, `%in%`, which matches a vector to another. In the code below `%in%` will return `FALSE`, `FALSE`, `TRUE`. This is because within `some_vec`, `1` and `2` are not found within s`ome_other_vector` but `3` is found and returns `TRUE`. The `%in%` is useful to find matches.

>some_vec <- c(1, 2, 3)
some_other_vector <- c(3, "a", "b")
some_vec %in% some_other_vector

Another new operator is `!`. For logical conditions, adding `!` will inverse the result. In the above example, the `FALSE`, `FALSE`, `TRUE` will become `TRUE`, `TRUE`, `FALSE`. Using it in concert with `%in%` will inverse the response and is good for removing items that are matched.

> !some_vec %in% some_other_vector

We've created `oz` which is the tidy version of The Wizard of Oz along with `nrc` containing the "NRC" lexicon with renamed columns.

***Instruction 1:***

- Inner join `oz` to the `nrc` lexicon.

  - Call `inner_join()` to join the tibbles.

  - Join `by` the `term` column in the text and the `word` column in the lexicon.

- Filter to only Pluchik's emotions and drop the positive or negative words in the lexicon.

  - Use `filter()` to keep rows where the `sentiment` is not `"positive"` or `"negative"`.

- Group by sentiment.

  - Call `group_by()`, passing `sentiment` without quotes.

- Get the total count of each sentiment.

  - Call `summarize()`, setting `total_count` equal to the `sum()` of count.

  - Assign the result to `oz_plutchik`.


```{r eval=FALSE}
oz_plutchik <- oz %>% 
  # Join to nrc lexicon by term = word
  inner_join(nrc, by = c("term" = "word")) %>% 
  # Only consider Plutchik sentiments
  filter(!sentiment %in% c("positive", "negative")) %>%
  # Group by sentiment
  group_by(sentiment) %>%
  # Get total count by sentiment
  summarize(total_count = sum(count))
```

***Instruction 2:***

- Create a bar plot with `ggplot()`.

  - Pass in `oz_plutchik` to the `data` argument.

  - Then specify the `x` and `y` aesthetics, calling `aes()` and passing `sentiment` and `total_count` without quotes.

  - Add a column geom with `geom_col()`. (This is the same as `geom_bar()`, but doesn't summarize the data, since you've done that already.)

```{r eval=FALSE}
# Plot total_count vs. sentiment
ggplot(oz_plutchik, aes(x = sentiment, y = total_count)) +
  # Add a column geom
  geom_col()
```
