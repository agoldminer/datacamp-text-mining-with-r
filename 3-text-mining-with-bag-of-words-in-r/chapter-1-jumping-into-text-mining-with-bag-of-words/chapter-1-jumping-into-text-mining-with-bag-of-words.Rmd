---
title: "chapter-1-jumping-into-text-mining-with-bag-of-words"
author: "Xiaotian Sun"
date: "11/02/2020"
output: html_document
---

## 1. Jumping into Text Mining with Bag of Words

##### 1.1 What is text mining? (video)

- The process of distilling actionable insights from text.

**Text mining workflow**

- 1. Problem definition & specific goals

- 2. Identify text to be collected

- 3. Text organization

- 4. Feature extraction

**Two measures*

- semantic parsing

- bag of words


##### 1.2 Understanding text mining

##### 1.3 Quick taste of text mining

Sometimes we can find out the author's intent and main ideas just by looking at the most common words.

At its heart, bag of words text mining represents a way to count terms, or n-grams, across a collection of documents. Consider the following sentences, which we've saved to `text` and made available in your workspace:

text <- "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."

Manually counting the words in the sentences above is a pain! Fortunately, the `qdap` package offers a better alternative. You can easily find the top 4 most frequent terms (including ties) in `text` by calling the `freq_terms` function and specifying `4`.

`frequent_terms <- freq_terms(text, 4)`

The `frequent_terms` object stores all unique words and their count. You can then make a bar chart simply by calling the `plot` function on the `frequent_terms` object.

`plot(frequent_terms)`

***Instruction :***

We've created an object in your workspace called `new_text` containing several sentences.

- Load the `qdap` package.

- Print `new_text` to the console.

- Create `term_count` consisting of the 10 most frequent terms in `new_text`.

- Plot a bar chart with the results of `term_count`.

```{r echo=FALSE}
new_text <- "DataCamp is the first online learning platform that focuses on building the best learning experience specifically for Data Science. We have offices in New York, London and Belgium and to date, we trained over 3.8 million (aspiring) data scientists in over 150 countries. These data science enthusiasts completed more than 185 million exercises. You can take free beginner courses, or subscribe for $29/month to get access to all premium courses."
```

```{r}
# Load qdap
library(qdap)

# Print new_text to the console
new_text

# Find the 10 most frequent terms: term_count
term_count <- freq_terms(new_text, 10)

# Plot term_count
plot(term_count)
```

##### 1.4 Getting started (video)

- Building our first corpus

##### 1.5 Load some text

Text mining begins with loading some text data into R, which we'll do with the `read.csv()` function. By default, `read.csv()` treats `character` strings as `factor` levels like `Male`/`Female`. To prevent this from happening, it's very important to use the argument `stringsAsFactors = FALSE`.

A best practice is to examine the object you read in to make sure you know which column(s) are important. The `str()` function provides an efficient way of doing this.

If the data frame contains columns that are not text, you may want to make a new object using only the correct column of text (e.g. `some_object$column_name`).

**Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data).**

***Instruction:***

The data has been loaded for you and is available in `coffee_data_file`.

- Create a new object `tweets` using `read.csv()` on the file `coffee_data_file`, which contains tweets mentioning coffee. Remember to add `stringsAsFactors = FALSE`!

- Examine the `tweets` object using `str()` to determine which column has the text you'll want to analyze.

- Make a new `coffee_tweets` object using only the text column you identified earlier. To do so, use the `$` operator and column name.

```{r}
# Import text data
tweets <- read.csv('coffee.csv', stringsAsFactors = F)

# View the structure of tweets
str(tweets)

# Isolate text from tweets: coffee_tweets
coffee_tweets <- tweets$text
```

##### 1.6 Make the vector a VCorpus object (1)

Recall that you've loaded your text data as a vector called `coffee_tweets` in the last exercise. Your next step is to convert this vector containing the text data to a corpus. As you've learned in the video, a corpus is a collection of documents, but it's also important to know that in the `tm` domain, R recognizes it as a data type.

There are two kinds of the corpus data type, the permanent corpus, `PCorpus`, and the volatile corpus, `VCorpus`. In essence, the difference between the two has to do with how the collection of documents is stored in your computer. In this course, we will use the volatile corpus, which is held in your computer's RAM rather than saved to disk, just to be more memory efficient.

To make a volatile corpus, R needs to interpret each element in our vector of text, `coffee_tweets`, as a document. And the `tm` package provides what are called Source functions to do just that! In this exercise, we'll use a Source function called `VectorSource()` because our text data is contained in a vector. The output of this function is called a Source object. Give it a shot!

***Instruction :***

- Load the `tm` package.

- Create a Source object from the `coffee_tweets` vector. Call this new object `coffee_source`.

```{r}
# Load tm
library(tm)

# Make a vector source from coffee_tweets
coffee_source <- VectorSource(coffee_tweets)
```

##### 1.7 Make the vector a VCorpus object (2) 

Now that we've converted our vector to a Source object, we pass it to another `tm` function, `VCorpus()`, to create our volatile corpus. Pretty straightforward, right?

The `VCorpus` object is a nested list, or list of lists. At each index of the `VCorpus` object, there is a `PlainTextDocument` object, which is a list containing actual text data (`content`), and some corresponding metadata (`meta`). It can help to visualize a `VCorpus` object to conceptualize the whole thing.

To review a single document object (the 10th) you subset with double square brackets.

`coffee_corpus[[10]]`

To review the actual text you index the list twice. To access the document's metadata, like timestamp, change `[1]` to `[2]`. Another way to review the plain text is with the `content()` function which doesn't need the second set of brackets.

`coffee_corpus[[10]][1]`

`content(coffee_corpus[[10]])`

***Instruction:***

- Call the `VCorpus()` function on the `coffee_source` object to create `coffee_corpus`.

- Verify `coffee_corpus` is a `VCorpus` object by printing it to the console.

- Print the 15th element of `coffee_corpus` to the console to verify that it's a `PlainTextDocument` that contains the content and metadata of the 15th tweet. Use double bracket subsetting.

- Print the content of the 15th tweet in `coffee_corpus`. Use double brackets to select the proper tweet, followed by single brackets to extract the content of that tweet.

- Print the `content()` of the 10th tweet within `coffee_corpus`

```{r}
## coffee_source is already in your workspace

# Make a volatile corpus: coffee_corpus
coffee_corpus <- VCorpus(coffee_source)

# Print out coffee_corpus
coffee_corpus

# Print the 15th tweet in coffee_corpus
coffee_corpus[[15]]

# Print the contents of the 15th tweet in coffee_corpus
coffee_corpus[[15]]$content

# Now use content to review plain text of the 10th tweet
content(coffee_corpus[[10]])
```

##### 1.8 Make a VCorpus from a data frame 

If your text data is in a data frame you can use `DataframeSource()` for your analysis. The data frame passed to `DataframeSource()` must have a specific structure:

- Column one must be called `doc_id` and contain a unique string for each row.

- Column two must be called text with `"UTF-8"` encoding (pretty standard).

- Any other columns, 3+ are considered metadata and will be retained as such.`

This exercise introduces `meta()` to extract the metadata associated with each document. Often your data will have metadata such as authors, dates, topic tags or places which can inform your analysis. Once your text is a corpus, you can apply `meta()` to examine the additional document level information.

***Instruction:***

In your workspace, there's a simple data frame called `example_text` with the correct column names and some metadata. There is also `vec_corpus` which is a volatile corpus made with `VectorSource()`

- Create `df_source` using `DataframeSource()` with the example_text.

- Create `df_corpus` by converting `df_source` to a volatile corpus object with `VCorpus()`.

- Print out `df_corpus`. Notice how many documents it contains and the number of retained document level metadata points.

- Use `meta()` on `df_corpus` to print the document associated metadata.

- Examine the pre-loaded `vec_corpus` object. Compare the number of documents to `df_corpus`.

- Use `meta()` on `vec_corpus` to compare any metadata found between `vec_corpus` and `df_corpus`.

```{r echo=FALSE}
example_text <- read.csv("example_text.csv",stringsAsFactors = F)

# Create a VectorSource on column 3: vec_source
vec_source <- VectorSource(example_text[, 3])

# Convert vec_source to a corpus: vec_corpus
vec_corpus <- VCorpus(vec_source)
```

```{r }
# Create a DataframeSource from the example text
df_source <- DataframeSource(example_text)

# Convert df_source to a volatile corpus
df_corpus <- VCorpus(df_source)

# Examine df_corpus
df_corpus

# Examine df_corpus metadata
meta(df_corpus)

# Compare the number of documents in the vector source
vec_corpus

# Compare metadata in the vector corpus
meta(vec_corpus)
```

##### 1.9 Cleaning and preprocessing text (video)

*TM Function*

- tolower(): make all text lowercase

- removePunctuation(): removes punctuation like periods and exclamation points

- removeNumbers(): removes numbers

- stripWhiteSpace(): removes tabs and extra spaces

- removeWords(): removes specific words(defined by the data scientist)

- tm_map()

- stemDocument()

##### 1.10 Common cleaning functions from tm 

Now that you know two ways to make a corpus, you can focus on cleaning, or preprocessing, the text. First, you'll clean a small piece of text, then you will move on to larger corpora.

In bag of words text mining, cleaning helps aggregate terms. For example, it might make sense for the words "miner", "mining" and "mine" to be considered one term. Specific preprocessing steps will vary based on the project. For example, the words used in tweets are vastly different than those used in legal documents, so the cleaning process can also be quite different.

Common preprocessing functions include:

- `tolower()`: Make all characters lowercase

- `removePunctuation()`: Remove all punctuation marks

- `removeNumbers()`: Remove numbers

- `stripWhitespace()`: Remove excess whitespace

`tolower()` is part of base R, while the other three functions come from the `tm` package. Going forward, we'll load `tm` and `qdap` for you when they are needed. Every time we introduce a new package, we'll have you load it the first time.

The variable text, containing a sentence, is shown in the script.

***Instruction:***

Apply each of the following functions to `text`, simply printing results to the console:

- `tolower()`

- `removePunctuation()`

- `removeNumbers()`

- `stripWhitespace()`

```{r}
# Create the object: text
text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."

# Make lowercase
tolower(text)

# Remove punctuation
removePunctuation(text)

# Remove numbers
removeNumbers(text)

# Remove whitespace
stripWhitespace(text)
```

##### 1.11 Cleaning with qdap 

The `qdap` package offers other text cleaning functions. Each is useful in its own way and is particularly powerful when combined with the others.

- `bracketX()`: Remove all text within brackets (e.g. "It's (so) cool" becomes "It's cool")

- `replace_number()`: Replace numbers with their word equivalents (e.g. "2" becomes "two")

- `replace_abbreviation()`: Replace abbreviations with their full text equivalents (e.g. "Sr" becomes "Senior")

- `replace_contraction()`: Convert contractions back to their base words (e.g. "shouldn't" becomes "should not")

- `replace_symbol()`: Replace common symbols with their word equivalents (e.g. "$" becomes "dollar")

***Instruction:***

Apply the following functions to the `text` object from the previous exercise:

- `bracketX()`

- `replace_number()`

- `replace_abbreviation()`

- `replace_contraction()`

- `replace_symbol()`

```{r echo=FALSE}
library(qdap)
```

```{r}
## text is still loaded in your workspace

# Remove text within brackets
bracketX(text)

# Replace numbers with words
replace_number(text)

# Replace abbreviations
replace_abbreviation(text)

# Replace contractions
replace_contraction(text)

# Replace symbols with words
replace_symbol(text)
```

##### 1.12 All about stop words

Often there are words that are frequent but provide little information. These are called stop words, and you may want to remove them from your analysis. Some common English stop words include "I", "she'll", "the", etc. In the `tm` package, there are 174 common English stop words (you'll print them in this exercise!)

When you are doing an analysis you will likely need to add to this list. In our coffee tweet example, all tweets contain "coffee", so it's important to pull out that word in addition to the common stop words. Leaving "coffee" in doesn't add any insight and will cause it to be overemphasized in a frequency analysis.

Using the `c()` function allows you to add new words to the stop words list. For example, the following would add "word1" and "word2" to the default list of English stop words:

`all_stops <- c("word1", "word2", stopwords("en"))`

Once you have a list of stop words that makes sense, you will use the `removeWords()` function on your text.` removeWords()` takes two arguments: the `text` object to which it's being applied and the list of words to remove.

***Instruction:***

- Review standard stop words by calling `stopwords("en")`.

- Remove "en" stopwords from `text`.

- Add "coffee" and "bean" to the standard stop words, assigning to `new_stops`.

- Remove the customized stopwords, `new_stops`, from `text`.

```{r}
## text is preloaded into your workspace

# List standard English stop words
stopwords("en")

# Print text without standard stop words
removeWords(text, stopwords("en"))

# Add "coffee" and "bean" to the list: new_stops
new_stops <- c("coffee", "bean", stopwords("en"))

# Remove stop words from text
removeWords(text, new_stops)
```

##### 1.13 Intro to word stemming and stem completion

Still another useful preprocessing step involves word stemming and stem completion. Word stemming reduces words to unify across documents. For example, the stem of "computational", "computers" and "computation" is "comput". But because "comput" isn't a real word, we want to re-construct the words so that "computational", "computers", and "computation" all refer a recognizable word, such as "computer". The reconstruction step is called stem completion.

The `tm` package provides the `stemDocument()` function to get to a word's root. This function either takes in a character vector and returns a character vector, or takes in a `PlainTextDocument` and returns a `PlainTextDocument`.

For example,

`stemDocument(c("computational", "computers", "computation"))`
returns `"comput" "comput" "comput"`

You will use `stemCompletion()` to reconstruct these word roots back into a known term. `stemCompletion()` accepts a character vector and a completion dictionary. The completion dictionary can be a character vector or a Corpus object. Either way, the completion dictionary for our example would need to contain the word "computer" so all instances of "comput" can be reconstructed.

***Instruction:***

- Create a vector called `complicate` consisting of the words "complicated", "complication", and "complicatedly" in that order.

- Store the stemmed version of `complicate` to an object called `stem_doc`.

- Create comp_dict that contains one word, "complicate".

- Create `complete_text` by applying `stemCompletion()` to `stem_doc`. Re-complete the words using `comp_dict` as the reference corpus.

- Print `complete_text` to the console.

```{r}
# Create complicate
complicate <- c("complicated", "complication", "complicatedly")

# Perform word stemming: stem_doc
stem_doc <- stemDocument(complicate)

# Create the completion dictionary: comp_dict
comp_dict <- c("complicate")

# Perform stem completion: complete_text 
complete_text <- stemCompletion(stem_doc, comp_dict)

# Print complete_text
complete_text
```

##### 1.14 Word stemming and stem completion on a sentence

Let's consider the following sentence as our document for this exercise:

`"In a complicated haste, Tom rushed to fix a new complication, too complicatedly."`

This sentence contains the same three forms of the word "complicate" that we saw in the previous exercise. The difference here is that even if you called `stemDocument()` on this sentence, it would return the sentence without stemming any words. Take a moment and try it out in the console. Be sure to include the punctuation marks.

This happens because `stemDocument()` treats the whole sentence as one word. In other words, our document is a character vector of length 1, instead of length n, where n is the number of words in the document. To solve this problem, we first remove the punctuation marks with the removePunctuation() function you learned a few exercises back. We then `strsplit()` this character vector of length 1 to length n, `unlist()`, then proceed to stem and re-complete.

Don't worry if that was confusing. Let's go through the process step by step!

***Instruction:***

The document `text_data` and the completion dictionary `comp_dict` are loaded in your workspace.

- Remove the punctuation marks in `text_data` using `removePunctuation()`, assigning to `rm_punc`.

- Call `strsplit()` on `rm_punc` with the `split` argument set equal to `" "`. Nest this inside `unlist()`, assigning to `n_char_vec`.

- Use `stemDocument()` again to perform word stemming on `n_char_vec`, assigning to `stem_doc`.

- Create `complete_doc` by re-completing your stemmed document with `stemCompletion()` and using `comp_dict` as your reference corpus.

Are `stem_doc` and `complete_doc` what you expected?

```{r echo=FALSE}
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."
```

```{r}
# Remove punctuation: rm_punc
rm_punc <- removePunctuation(text_data)

# Create character vector: n_char_vec
n_char_vec <- unlist(strsplit(rm_punc, split = ' '))

# Perform word stemming: stem_doc
stem_doc <- stemDocument(n_char_vec) 

# Print stem_doc
stem_doc

# Re-complete stemmed document: complete_doc
complete_doc <- stemCompletion(stem_doc, comp_dict)

# Print complete_doc
complete_doc
```

##### 1.15 Apply preprocessing steps to a corpus 

The `tm` package provides a function `tm_map()` to apply cleaning functions to an entire corpus, making the cleaning steps easier.

`tm_map()` takes two arguments, a corpus and a cleaning function. Here, `removeNumbers()` is from the `tm` package.

`corpus <- tm_map(corpus, removeNumbers)`

For compatibility, base R and `qdap` functions need to be wrapped in `content_transformer()`.

`corpus <- tm_map(corpus, content_transformer(replace_abbreviation))`

You may be applying the same functions over multiple corpora; using a custom function like the one displayed in the editor will save you time (and lines of code). `clean_corpus()` takes one argument, `corpus`, and applies a series of cleaning functions to it in order, then returns the updated corpus.

The order of cleaning steps makes a difference. For example, if you `removeNumbers()` and then `replace_number()`, the second function won't find anything to change! **Check, check, and re-check your results!**

***Instruction 1:***

Edit the custom function `clean_corpus()` in the sample code to apply (in order):
  
- `tm`'s `removePunctuation()`.
  
- Base R's `tolower()`.
  
- Append `"mug"` to the stop words list.
  
- `tm`'s `stripWhitespace()`.

```{r}
# Alter the function code to match the instructions
clean_corpus <- function(corpus) {
  # Remove punctuation
  corpus <- tm_map(corpus, removePunctuation)
  # Transform to lower case
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Add more stopwords
  corpus <- tm_map(corpus, removeWords, words = c(stopwords("en"), "coffee", "mug"))
  # Strip whitespace
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}
```

***Instruction 2:***

- Create `clean_corp` by applying `clean_corpus()` to the included corpus `tweet_corp`.

- Print the cleaned 227th tweet in `clean_corp` using indexing `[[227]]` and `content()`.

- Compare it to the original tweet from `tweets$text` tweet using `[227]`.

```{r echo=FALSE}
tweet_corp <- coffee_corpus
```

```{r}
# Apply your customized function to the tweet_corp: clean_corp
clean_corp <- clean_corpus(tweet_corp)

# Print out a cleaned up tweet
clean_corp[[227]][1]

# Print out the same tweet in original form
tweet_corp[[227]][1]
```

##### 1.16 The TDM & DTM (video)

- TMD: term document matrix, words as rows and documents as columns

- DTM: document term matrix, documents as rows and words as columns

##### 1.17 Understanding TDM and DTM

##### 1.18 Make a document-term matrix

Hopefully you are not too tired after all this basic text mining work! Just in case, let's revisit the coffee and get some Starbucks while building a document-term matrix from coffee tweets.

Beginning with the `coffee.csv` file, we have used common transformations to produce a clean corpus called `clean_corp`.

The document-term matrix is used when you want to have each document represented as a row. This can be useful if you are comparing authors within rows, or the data is arranged chronologically and you want to preserve the time series. The tm package uses a "simple triplet matrix" class. However, it is often easier to manipulate and examine the object by re-classifying the DTM with `as.matrix()`

***Instruction:***

- Create `coffee_dtm` by applying `DocumentTermMatrix()` to `clean_corp`.

- Create `coffee_m`, a matrix version of `coffee_dtm`, using `as.matrix()`.

- Print the dimensions of `coffee_m` to the console using the `dim()` function. Note the number of rows and columns.

- Print the subset of `coffee_m` containing documents (rows) 25 through 35 and terms (columns) `"star"` and `"starbucks"`.


```{r}
# Create the document-term matrix from the corpus
coffee_dtm <- DocumentTermMatrix(clean_corp)

# Print out coffee_dtm data
coffee_dtm

# Convert coffee_dtm to a matrix: coffee_m
coffee_m <- as.matrix(coffee_dtm)

# Print the dimensions of coffee_m
dim(coffee_m)

# Review a portion of the matrix to get some Starbucks
coffee_m[25:35, c("star", "starbucks")]
```

##### 1.19 Make a term-document matrix 

You're almost done with the not-so-exciting foundational work before we get to some fun visualizations and analyses based on the concepts you've learned so far!

In this exercise, you are performing a similar process but taking the transpose of the document-term matrix. In this case, the term-document matrix has terms in the first column and documents across the top as individual column names.

The TDM is often the matrix used for language analysis. This is because you likely have more terms than authors or documents and life is generally easier when you have more rows than columns. An easy way to start analyzing the information is to change the matrix into a simple matrix using `as.matrix()` on the TDM.

***Instruction:***

- Create `coffee_tdm` by applying `TermDocumentMatrix()` to `clean_corp`.

- Create `coffee_m` by converting `coffee_tdm` to a matrix using `as.matrix()`.

- Print the dimensions of `coffee_m` to the console. Note the number of rows and columns.

- Print the subset of `coffee_m` containing terms (rows) `"star"` and `"starbucks"` and documents (columns) 25 through 35.

```{r}
# Create a term-document matrix from the corpus
coffee_tdm <- TermDocumentMatrix(clean_corp)

# Print coffee_tdm data
coffee_tdm

# Convert coffee_tdm to a matrix: coffee_m
coffee_m <- as.matrix(coffee_tdm)

# Print the dimensions of the matrix
dim(coffee_m)

# Review a portion of the matrix
coffee_m[c("star", "starbucks"), 25:35]
```
