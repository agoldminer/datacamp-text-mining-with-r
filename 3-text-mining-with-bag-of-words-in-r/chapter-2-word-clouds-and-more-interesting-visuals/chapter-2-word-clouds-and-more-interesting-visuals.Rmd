---
title: "chapter-2-word-clouds-and-more-interesting-visuals"
author: "Xiaotian Sun"
date: "14/02/2020"
output: html_document
---

## 2. Word Clouds and More Interesting Visuals

##### 2.1 Common text mining visuals (video)

- term frequency plots with tm

- term frequency plots with qdap

##### 2.2 Test your understanding of text mining 

##### 2.3 Frequent terms with tm

Now that you know how to make a term-document matrix, as well as its transpose, the document-term matrix, we will use it as the basis for some analysis. In order to analyze it we need to change it to a simple matrix like we did in chapter 1 using `as.matrix()`.

Calling `rowSums()` on your newly made matrix aggregates all the terms used in a passage. Once you have the `rowSums()`, you can `sort()` them with `decreasing = TRUE`, so you can focus on the most common terms.

Lastly, you can make a `barplot()` of the top 5 terms of `term_frequency` with the following code.

`barplot(term_frequency[1:5], col = "#C0DE25")`

Of course, you could take our ggplot2 course to learn how to customize the plot even more... :)

***Instruction:***

- Create `coffee_m` as a matrix using the term-document matrix `coffee_tdm` from the last chapter.

- Create `term_frequency` using the `rowSums()` function on `coffee_m`.

- Sort `term_frequency` in descending order and store the result in `term_frequency`.

- Use single square bracket subsetting, i.e. using only one `[`, to print the top 10 terms from `term_frequency`.

- Make a barplot of the top 10 terms.

```{r echo=FALSE}
library(tm)
tweets <- read.csv('coffee.csv', stringsAsFactors = F)
coffee_tweets <- tweets$text
coffee_source <- VectorSource(coffee_tweets)
coffee_corpus <- VCorpus(coffee_source)
clean_corpus <- function(corpus) {
  # Remove punctuation
  corpus <- tm_map(corpus, removePunctuation)
  # Transform to lower case
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Add more stopwords
  corpus <- tm_map(corpus, removeWords, words = c(stopwords("en"), "coffee", "mug"))
  # Strip whitespace
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}
clean_corp <- clean_corpus(coffee_corpus)
coffee_tdm <- TermDocumentMatrix(clean_corp)
```

```{r}
## coffee_tdm is still loaded in your workspace

# Convert coffee_tdm to a matrix
coffee_m <- as.matrix(coffee_tdm)

# Calculate the row sums of coffee_m
term_frequency <- rowSums(coffee_m)

# Sort term_frequency in decreasing order
term_frequency <- sort(term_frequency, decreasing = T)

# View the top 10 most common words
term_frequency[1:10]

# Plot a barchart of the 10 most common words
barplot(term_frequency[1:10], col = "tan", las = 2)
```

##### 2.4 Frequent terms with qdap 

If you are OK giving up some control over the exact preprocessing steps, then a fast way to get frequent terms is with `freq_terms()` from `qdap`.

The function accepts a text variable, which in our case is the `tweets$text` vector. You can specify the top number of terms to show with the top argument, a vector of stop words to remove with the `stopwords` argument, and the minimum character length of a word to be included with the `at.least` argument. `qdap` has its own list of stop words that differ from those in `tm`. Our exercise will show you how to use either and compare their results.

Making a basic plot of the results is easy. Just call `plot()` on the `freq_terms()` object.

***Instruction 1:***

- Create `frequency` using the `freq_terms()` function on `tweets$text`. Include arguments to accomplish the following:

  - Limit to the top 10 terms.
  
  - At least 3 letters per term.
  
  - Use `"Top200Words"` to define stop words.
  
- Produce a `plot()` of the `frequency` object. Compare it to the plot you produced in the previous exercise.

```{r echo=FALSE}
library(qdap)
```

```{r}
# Create frequency
frequency <- freq_terms(
  tweets$text, 
  top = 10, 
  at.least = 3, 
  stopwords = "Top200Words"
)

# Make a frequency barchart
plot(frequency)
```

***Instruction 2:***

- Again, create `frequency` using the `freq_terms()` function on `tweets$text`. Include the following arguments:

  - Limit to the top 10 terms.
  
  - At least 3 letters per term.
  
  - This time use `stopwords("english")` to define stop words.
  
- Produce a `plot()` of `frequency`. Compare it to the plot of `frequency`. Do certain words change based on the stop words criterion?

```{r}
# Create frequency
frequency <- freq_terms(tweets$text,
  top = 10,
  at.least = 3, 
  stopwords = stopwords("english")
  )

# Make a frequency barchart
plot(frequency)
```

##### 2.5 Intro to word clouds (video)

- a simple word cloud

- the impact of stop words

- removing uninformative words.

##### 2.6 A simple word cloud

At this point you have had too much coffee. Plus, seeing the top words such as "shop", "morning", and "drinking" among others just isn't all that insightful.

In celebration of making it this far, let's try our hand on another batch of 1000 tweets. For now, you won't know what they have in common, but let's see if you can figure it out using a word cloud. The tweets' term frequency values are preloaded in your workspace.

A word cloud is a visualization of terms. In a word cloud, size is often scaled to frequency and in some cases the colors may indicate another measurement. For now, we're keeping it simple: size is related to individual word frequency and we are just selecting a single color.

As you saw in the video, the `wordcloud()` function works like this:

`wordcloud(words, frequencies, max.words = 500, colors = "blue")`

Text mining analyses often include simple word clouds. In fact, they are probably over used, but can still be useful for quickly understanding a body of text!

`term_frequency` is loaded into your workspace.

***Instruction:***

- Load the `wordcloud` package.

- Print out first 10 entries in `term_frequency`.

- Extract the terms using `names()` on `term_frequency`. Call the vector of strings `terms_vec`.

- Create a `wordcloud()` using `terms_vec` as the words, and `term_frequency` as the values. Add the parameters `max.words = 50` and `colors = "red"`.

```{r echo=FALSE}
# Import chardonnay tweet data
chardonnay_tweets <- read.csv('chardonnay.csv', stringsAsFactors = F)

# Make a vector source
chardonnay_source <- VectorSource(chardonnay_tweets$text)

# Make a volatile corpus
chardonnay_corpus <- VCorpus(chardonnay_source)

# Clean the corpus
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, stopwords("en"))
  return(corpus)
}

chardonnay_corp <- clean_corpus(chardonnay_corpus)

# Convert TDM to matrix
chardonnay_tdm <- TermDocumentMatrix(chardonnay_corpus)
chardonnay_m <- as.matrix(chardonnay_tdm)

# Sum rows and frequency data frame
term_frequency <- rowSums(chardonnay_m)
```

```{r}
# Load wordcloud package
library(wordcloud)

# Print the first 10 entries in term_frequency
term_frequency[1:10]

# Vector of terms
terms_vec <- names(term_frequency)

# Create a wordcloud for the values in word_freqs
wordcloud(terms_vec, term_frequency, max.words = 50, colors = "red")
```

##### 2.7 Stop words and word clouds

Now that you are in the text mining mindset, sitting down for a nice glass of chardonnay, we need to dig deeper. In the last word cloud, "chardonnay" dominated the visual. It was so dominant that you couldn't draw out any other interesting insights.

Let's change the stop words to include "chardonnay" to see what other words are common, yet were originally drowned out.

Your workspace has a cleaned version of chardonnay tweets but now lets remove some non-insightful terms. This exercise uses `content()` to show you a specific tweet for comparison. Remember to use double brackets to index the corpus list.

***Instruction:***

- Apply `content()` to the 24th document in `chardonnay_corp`.

- Append `"chardonnay"` to the English stopwords, assigning to `stops`.

- Examine the last 6 words in `stops`.

- Create `cleaned_chardonnay_corp` with `tm_map()` by passing in the `chardonnay_corp`, the function `removeWords` and finally the stopwords, `stops`.

- Now examine the `content` of the `24` tweet again to compare results.

```{r}
# Review a "cleaned" tweet
content(chardonnay_corp[[24]])

# Add to stopwords
stops <- c(stopwords(kind = 'en'), 'chardonnay')

# Review last 6 stopwords 
tail(stops)

# Apply to a corpus
cleaned_chardonnay_corp <- tm_map(chardonnay_corp, removeWords, stops)

# Review a "cleaned" tweet again
content(cleaned_chardonnay_corp[[24]])
```

##### 2.8 Plot the better word cloud

Now that you've removed additional stopwords, let's take a look at the improved word cloud!

The term-document matrix from the previous exercise has been turned into matrix with `as.matrix()`, then a named vector was created with `rowSums()`. This new object of term frequencies called `chardonnay_words` is preloaded into your workspace. Let's take a look at these new wordcloud results.

***Instruction:***

We've loaded the `wordcloud` package for you behind the scenes and will do so for all additional exercises requiring it.

- Sort the values in `chardonnay_words` with `decreasing = TRUE`. Save as `sorted_chardonnay_words`.

- Look at the top 6 words in `sorted_chardonnay_words` and their values.

- Create `terms_vec` using `names()` on `chardonnay_words`.

- Pass `terms_vec` and `chardonnay_words` into the `wordcloud()` function. Review what other words pop out now that "chardonnay" is removed.

```{r echo=FALSE}
# Convert TDM to matrix
chardonnay_tdm <- TermDocumentMatrix(cleaned_chardonnay_corp)
chardonnay_m <- as.matrix(chardonnay_tdm)

# Sum rows and frequency data frame
term_frequency <- rowSums(chardonnay_m)
chardonnay_words <- rowSums(chardonnay_m)
chardonnay_words <- sort(chardonnay_words, decreasing = T)
```

```{r}
# Sort the chardonnay_words in descending order
sorted_chardonnay_words <- sort(chardonnay_words, decreasing = TRUE)

# Print the 6 most frequent chardonnay terms
head(sorted_chardonnay_words)

# Get a terms vector
terms_vec <- names(chardonnay_words)

# Create a wordcloud for the values in word_freqs
wordcloud(terms_vec, chardonnay_words, max.words = 50, colors = "red")
```

##### 2.9 Improve word cloud colors

So far, you have specified only a single hexadecimal color to make your word clouds. You can easily improve the appearance of a word cloud. Instead of the `#AD1DA5` in the code below, you can specify a vector of colors to make certain words stand out or to fit an existing color scheme.

< wordcloud(chardonnay_freqs$term, 
          chardonnay_freqs$num, 
          max.words = 100, 
          colors = "#AD1DA5")
          
To change the `colors` argument of the `wordcloud()` function, you can use a vector of named colors like `c("chartreuse", "cornflowerblue", "darkorange")`. The function `colors()` will list all 657 named colors. You can also use this PDF as a reference.

In this exercise you will use "grey80", "darkgoldenrod1", and "tomato" as colors. This is a good starting palette to highlight terms because "tomato" stands out more than "grey80". It is a best practice to start with three colors, each with increasing vibrancy. Doing so will naturally divide the term frequency into "low", "medium" and "high" for easier viewing.

***Instruction:***

- Call the `colors()` function to list all basic colors.

- Create a `wordcloud()` using the predefined `chardonnay_freqs` with the colors "grey80", "darkgoldenrod1", and "tomato". Include the top 100 terms using `max.words`.

```{r echo=FALSE}
chardonnay_freqs <- data.frame(
  term = names(chardonnay_words),
  num = chardonnay_words
  )
```

```{r}
# Print the list of colors
# colors()

# Print the wordcloud with the specified colors
wordcloud(chardonnay_freqs$term, 
          chardonnay_freqs$num,
          max.words = 100, 
          colors = c("grey80","darkgoldenrod1", "tomato"))
```

##### 2.10 Use prebuilt color palettes

In celebration of your text mining skills, you may have had too many glasses of chardonnay while listening to Marvin Gaye. So if you find yourself unable to pick colors on your own, you can use the `viridisLite` package. `viridisLite` color schemes are perceptually-uniform, both in regular form and when converted to black-and-white. The colors are also designed to be perceived by readers with color blindness.

There are multiple color palettes each with a convenience function. Simply specify `n` to select the number of colors needed.

>magma(n = 3)
plasma(n = 5)
inferno(n = 6)

Each function returns a vector of hexadecimal colors based on `n`. Here's an example used with wordcloud:

>color_pal <- cividis(n = 7)
wordcloud(chardonnay_freqs$term, chardonnay_freqs$num, max.words = 100, colors = color_pal)

***Instruction:***

- Use `cividis()` to select `5` colors in an object called `color_pal`.

- Review the hexadecimal colors by printing `color_pal` to your console.

- Create a `wordcloud()` from the `chardonnay_freqs` `term` and `num` columns. Include the top 100 terms using `max.words`, and set the `colors` to your palette, `color_pal`.

```{r echo=FALSE}
library(viridisLite)
```

```{r}
# Select 5 colors 
color_pal <- cividis(5)

# Examine the palette output
color_pal

# Create a wordcloud with the selected palette
wordcloud(chardonnay_freqs$term, 
          chardonnay_freqs$num,
          max.words = 100, 
          colors = color_pal)
```

##### 2.11 Other word clouds and word networks (video)

- commonality clouds

- comparison clouds

- pyramid plots

- word networksa

##### 2.12 Find common words

Say you want to visualize common words across multiple documents. You can do this with `commonality.cloud()`.

Each of our coffee and chardonnay corpora is composed of many individual tweets. To treat the coffee tweets as a single document and likewise for chardonnay, you `paste()` together all the tweets in each corpus along with the parameter `collapse = " "`. This collapses all tweets (separated by a space) into a single vector. Then you can create a single vector containing the two collapsed documents.

`a_single_string <- paste(a_character_vector, collapse = " ")`

Once you're done with these steps, you can take the same approach you've seen before to create a VCorpus() based on a VectorSource from the all_tweets object.

***Instruction:***

- Create `all_coffee` by using `paste()` with `collapse = " "` on `coffee_tweets$text`.

- Create `all_chardonnay` by using `paste()` with `collapse = " "` on `chardonnay_tweets$text`.

- Create `all_tweets` using `c()` to combine `all_coffee` and `all_chardonnay`. Make `all_coffee` the first term.
- Convert `all_tweets` using `VectorSource()`.

- Create `all_corpus` by using `VCorpus()` on `all_tweets`.

```{r}
# Create all_coffee
all_coffee = paste(coffee_tweets, collapse=" ")

# Create all_chardonnay
all_chardonnay = paste(chardonnay_tweets$text, collapse=" ")

# Create all_tweets
all_tweets = c(all_coffee,all_chardonnay)

# Convert to a vector source
all_tweets = VectorSource(all_tweets)

# Create all_corpus
all_corpus = VCorpus(all_tweets)
```

##### 2.13 Visualize common words

Now that you have a corpus filled with words used in both the chardonnay and coffee tweets files, you can clean the corpus, convert it into a `TermDocumentMatrix`, and then a matrix to prepare it for a `commonality.cloud()`.

The `commonality.cloud()` function accepts this matrix object, plus additional arguments like `max.words` and `colors` to further customize the plot.

>commonality.cloud(tdm_matrix, max.words = 100, colors = "springgreen")

***Instruction:***

- Create `all_clean` by applying the predefined `clean_corpus()` function to `all_corpus`.

- Create `all_tdm`, a `TermDocumentMatrix` from all_clean.

- Create `all_m` by converting `all_tdm` to a matrix object.

- Create a `commonality.cloud()` from `all_m` with `max.words = 100` and `colors = "steelblue1"`.

```{r echo=FALSE}
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, 
    c(stopwords("en"), "amp", "chardonnay", "wine", "glass", "coffee"))
  return(corpus)
}
```

```{r}
# Clean the corpus
all_clean <- clean_corpus(all_corpus)

# Create all_tdm
all_tdm <- TermDocumentMatrix(all_clean)

# Create all_m
all_m <- as.matrix(all_tdm)

# Print a commonality cloud
commonality.cloud(all_m, max.words = 100, colors = "steelblue1")
```

##### 2.14 Visualize dissimilar words

Say you want to visualize the words not in common. To do this, you can also use `comparison.cloud()` and the steps are quite similar with one main difference.

Like when you were searching for words in common, you start by unifying the tweets into distinct corpora and combining them into their own `VCorpus()` object. Next apply a `clean_corpus()` function and organize it into a `TermDocumentMatrix`.

To keep track of what words belong to `coffee` versus `chardonnay`, you can set the column names of the TDM like this:

>colnames(all_tdm) <- c("chardonnay", "coffee")

Lastly, convert the object to a matrix using `as.matrix()` for use in `comparison.cloud()`. For every distinct corpora passed to the comparison.cloud() you can specify a color, as in `colors = c("red", "yellow", "green")`, to make the sections distinguishable.

***Instruction:***

`all_corpus` is preloaded in your workspace.

- Create `all_clean` by applying the predefined `clean_corpus` function to `all_corpus`.

- Create `all_tdm`, a `TermDocumentMatrix`, from `all_clean`.

- Use `colnames()` to rename each distinct corpora within `all_tdm`. Name the first column "coffee" and the second column "chardonnay".

- Create `all_m` by converting `all_tdm` into matrix form.

- Create a `comparison.cloud()` using `all_m`, with `colors = c("orange", "blue")` and `max.words = 50`.

```{r warning=FALSE}
# Clean the corpus
all_clean <- clean_corpus(all_corpus)

# Create all_tdm
all_tdm <- TermDocumentMatrix(all_clean)

# Give the columns distinct names
colnames(all_tdm) <- c("coffee", "chardonnay")

# Create all_m
all_m <- as.matrix(all_tdm)

# Create comparison cloud
comparison.cloud(all_m,
                 colors = c("orange", "blue"),
                 max.words = 50)
```

##### 2.15 Polarized tag cloud

Commonality clouds show words that are shared across documents. One interesting thing that they can't show you is which of those words appear more commonly in one document compared to another. For this, you need a pyramid plot; these can be generated using `pyramid.plot()` from the `plotrix` package.

First, some manipulation is required to get the data in a suitable form. This is most easily done by converting it to a data frame and using dplyr. Given a matrix of word counts, as created by `as.matrix(tdm`), you need to end up with a data frame with three columns:

- The words contained in each document.

- The counts of those words from document 1.

- The counts of those words from document 2.

- Then pyramid.plot() using

pyramid.plot(word_count_data$count1, word_count_data$count2, word_count_data$word)
There are some additional arguments to improve the cosmetic appearance of the plot.

Now you'll explore words that are common in chardonnay tweets, but rare in coffee tweets. all_dtm_m is created for you.

***Instruction 1:***

- Convert `all_tdm_m` to a data frame. Set the rownames to a column named `"word"`.

- Filter to keep rows where all columns are greater than zero, using the syntax `. > 0`.

- Add a column named `difference`, equal to the count in the `chardonnay` column minus the count in the `coffee` column.

- Filter to keep the top `25` rows by `difference`.

- Arrange the rows by `desc()`ending order of `difference`.

```{r eval = FALSE}
top25_df <- all_tdm_m %>%
  # Convert to data frame
  as_data_frame(rownames = "word") %>% 
  # Keep rows where word appears everywhere
  filter_all(all_vars(. > 0)) %>% 
  # Get difference in counts
  mutate(difference = chardonnay - coffee) %>% 
  # Keep rows with biggest difference
  top_n(25, wt = difference) %>% 
  # Arrange by descending difference
  arrange(desc(difference))
```

***Instruction 2:***

- Set the left count to the `chardonnay` column.

- Set the right count to the `coffee` column.

- Set the labels to the `word` column.

```{r eval = FALSE}
pyramid.plot(
  # Chardonnay counts
  top25_df$chardonnay, 
  # Coffee counts
  top25_df$coffee, 
  # Words
  labels = top25_df$word, 
  top.labels = c("Chardonnay", "Words", "Coffee"), 
  main = "Words in Common", 
  unit = NULL,
  gap = 8,
)
```

##### 2.16 Visualize word networks

Another way to view word connections is to treat them as a network, similar to a social network. Word networks show term association and cohesion. A word of caution: these visuals can become very dense and hard to interpret visually.

In a network graph, the circles are called nodes and represent individual terms, while the lines connecting the circles are called edges and represent the connections between the terms.

For the over-caffeinated text miner, `qdap` provides a shorcut for making word networks. The `word_network_plot()` and `word_associate()` functions both make word networks easy!

The sample code constructs a word network for words associated with "Marvin".

***Instruction:***

Update the `word_associate()` plotting code to work with the coffee data.

- Change the vector to `coffee_tweets$text`.

- Change the match string to `"barista"`.

- Change `"chardonnay"` to `"coffee"` in the stopwords too.

- Change the title to `"Barista Coffee Tweet Associations"` in the sample code for the plot.

```{r warning=FALSE}
# Word association
word_associate(coffee_tweets, match.string = "barista", 
               stopwords = c(Top200Words, "coffee", "amp"), 
               network.plot = TRUE, cloud.colors = c("gray85", "darkred"))

# Add title
title(main = "Barista Coffee Tweet Associations")
```

##### 2.17 Teaser simple word clustering 
***Instruction:***

A hierarchical cluster object, `hc`, has been created for you from the coffee tweets.

Create a dendrogram using `plot()` on `hc`.

```{r echo=FALSE}
coffee_tdm2 <- removeSparseTerms(coffee_tdm, sparse = 0.975)

hc <- hclust(d = dist(coffee_tdm2, method = "euclidean"), method = "complete")
```


```{r}
# Plot a dendrogram
plot(hc)
```
